name: Benchmarks

on:
  push:
    branches:
      - main
    tags: "*"
    paths: &paths
      - ".github/workflows/Documentation.yml"
      - "src/**"
      - "Project.toml"
  pull_request:
    paths: *paths

concurrency:
  # Skip intermediate builds: always.
  # Cancel intermediate builds: always.
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  run-benchmarks:
    permissions:
      actions: write
      contents: write
      pull-requests: read
      statuses: write
    runs-on: aws-linux-nvidia-gpu-l4
    defaults:
      run:
        shell: bash
        working-directory: ./benchmarking
    container:
      # TODO: use image for benchmark environment after this is merged on `main`
      image: 'ghcr.io/numericalearth/breeze-docker-images:docs-julia_1.12.4'
      options: --gpus=all
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v6
      - name: Instantiate benchmarking environment
        shell: julia --color=yes --project {0}
        run: |
          using Pkg
          Pkg.instantiate()
      - name: Run benchmark
        timeout-minutes: 45
        run:
          earlyoom -m 3 -s 100 -r 300 --prefer 'julia' &
          julia --color=yes --project run_benchmarks.jl --device GPU --size "688x688x340"
      - uses: actions/upload-artifact@v6
        with:
          name: benchmarks-results
          path: benchmarking/benchmark_results.*
          retention-days: 90
      - name: Create job summary
        timeout-minutes: 1
        run:
          cat "benchmark_results.md" >> "${GITHUB_STEP_SUMMARY}"
      - name: Create JSON for github-action-benchmark
        shell: julia --color=yes --project {0}
        run: |
          # This step creates a new JSON file to be digested by the
          # `benchmark-action/github-action-benchmark` workflow.
          using JSON
          data = first(JSON.parse(read("benchmark_results.json")))
          dict = Dict(
              "name" => data["name"],
              "unit" => "steps/s",
              "value" => data["steps_per_second"],
          )
          open("github-action-benchmark.json", "w") do io
              JSON.json(io, dict)
          end
      - uses: actions/create-github-app-token@v2
        id: generate_token
        with:
          app-id: "${{ secrets.NUMTERRA_BOT_ID }}"
          private-key: "${{ secrets.NUMTERRA_BOT_PRIVATE_KEY }}"
          owner: NumericalEarth
          repositories: |
            Breeze.jl
            BreezeBenchmarks
      - name: Publish benchmark results
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: Breeze.jl Benchmarks
          tool: "customSmallerIsBetter"
          output-file-path: "benchmarking/github-action-benchmark.json"
          benchmark-data-dir-path: "benchmarks"
          gh-repository: "github.com/NumericalEarth/BreezeBenchmarks"
          github-token: ${{ steps.generate_token.outputs.token }}
          comment-always: true
          summary-always: true
          alert-threshold: "150%"
          fail-on-alert: false
          auto-push: true
          max-items-in-chart: 50
