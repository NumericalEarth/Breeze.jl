name: Benchmarks

on:
  push:
    branches:
      - main
    paths: &paths
      - ".github/workflows/Benchmarks.yml"
      - "benchmarking/**"
      - "src/**"
      - "Project.toml"
  pull_request:
    paths: *paths

concurrency:
  # Skip intermediate builds: always.
  # Cancel intermediate builds: always.
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # Disable GPU support for Reactant until we actually use it.
  REACTANT_GPU: 'none'

jobs:
  run-benchmarks:
    name: "Run benchmarks - dynamics ${{ matrix.dynamics }} - microphysics ${{ matrix.microphysics }} - grid ${{ matrix.grid }}"
    permissions:
      actions: write
      contents: write
      pull-requests: read
      statuses: write
    runs-on: aws-linux-nvidia-gpu-l4
    strategy:
      fail-fast: false
      matrix:
        include:
          - dynamics: 'anelastic'
            microphysics: 'nothing'
            grid: '128^3, 512x512x256, 768x768x256'
          - dynamics: 'compressible_splitexplicit'
            microphysics: 'nothing'
            grid: '512x512x256'
          - dynamics: 'anelastic'
            microphysics: 'MixedPhaseEquilibrium, 1M_MixedEquilibrium, 1M_MixedNonEquilibrium'
            grid: '512x512x256'
    defaults:
      run:
        shell: bash
        working-directory: ./benchmarking
    container:
      image: 'ghcr.io/numericalearth/breeze-docker-images:benchmarking-julia_1.12.4'
      options: --gpus=all
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v6
      - name: Instantiate benchmarking environment
        shell: julia --color=yes --project {0}
        run: |
          using Pkg
          Pkg.instantiate()
      - name: Run benchmarks
        timeout-minutes: 45
        run: |
          earlyoom -m 3 -s 100 -r 300 --prefer 'julia' &
          ARGS=()
          if [[ -n "${{ matrix.dynamics }}" ]]; then
              ARGS+=(--dynamics "${{ matrix.dynamics }}")
          fi
          if [[ -n "${{ matrix.microphysics }}" ]]; then
              ARGS+=(--microphysics "${{ matrix.microphysics }}")
          fi
          julia --color=yes --project run_benchmarks.jl --device GPU "${ARGS[@]}" --size "${{ matrix.grid }}"
      - uses: actions/upload-artifact@v6
        with:
          name: 'benchmarks-dynamics_${{ matrix.dynamics }}-microphysics_${{ matrix.microphysics }}-grid_${{ matrix.grid }}'
          path: benchmarking/benchmark_results.*
          retention-days: 90
          overwrite: false
      - name: Create job summary
        timeout-minutes: 1
        run:
          cat "benchmark_results.md" >> "${GITHUB_STEP_SUMMARY}"

  publish-benchmarks:
    name: "Publish benchmarks"
    needs: run-benchmarks
    permissions:
      actions: write
      contents: write
      pull-requests: write
      statuses: write
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/download-artifact@v7
        with:
          pattern: benchmarks-*
      - name: Merge results and create JSON for github-action-benchmark
        shell: python
        run: |
          # Merge benchmark_results.json from all artifact directories,
          # then create the JSON file for github-action-benchmark.
          import json, glob

          all_data = []
          for path in sorted(glob.glob('benchmarks-*/benchmark_results.json')):
              with open(path) as f:
                  all_data.extend(json.load(f))

          # Write merged results for archival
          with open('benchmark_results.json', 'w') as f:
              json.dump(all_data, f, indent=2)

          # Build github-action-benchmark entries
          # Name format: <name>/<properties>/<gpu>/<grid_size>
          data = []
          for d in all_data:
              base_name = d['name'].split('_')[0]
              advection = d['advection']
              # Closure is unused for the time being because it's always the
              # same everywhere in the current setup.
              closure = d['closure']
              dynamics = d['dynamics']
              ft = d['float_type']
              microphysics = d['microphysics']

              gpu = d['metadata']['gpu_name']

              grid = d['grid_size']
              grid_str = f"{grid[0]}x{grid[1]}x{grid[2]}"

              # We want to group benchmarks in two different ways: with the anelastic
              # dynamics, group all the 512x512x256 grids together, so that you can
              # compare the different microphysics.  Otherwise, if there's no microphysics
              # group together same dynamics _and_ microphysics.  NOTE: yes, same
              # benchmark may appear twice in both groups
              if dynamics == 'anelastic' and grid_str == '512x512x256':
                  level_1 = f'{base_name}; Dynamics: {dynamics}; Grid: {grid_str} [{ft}]'
                  level_2 = f'Advection: {advection}'
                  level_3 = f'{gpu}'
                  level_4 = f'{microphysics}'

                  name = f"{level_1}/{level_2}/{level_3}/{level_4}"
                  data.append({'name': name, 'unit': 'points/s', 'value': d['grid_points_per_second']})
                  pass

              if microphysics == 'nothing':
                  level_1 = f'{base_name}; Dynamics: {dynamics}; Microphysics: {microphysics} [{ft}]'
                  level_2 = f'Advection: {advection}'
                  level_3 = f'{gpu}'
                  level_4 = f'{grid_str}'

                  name = f"{level_1}/{level_2}/{level_3}/{level_4}"
                  data.append({'name': name, 'unit': 'points/s', 'value': d['grid_points_per_second']})

          with open('github-action-benchmark.json', 'w') as f:
              json.dump(data, f)
      - uses: actions/create-github-app-token@v2
        if: ${{ ! github.event.pull_request.head.repo.fork }}
        id: generate_token
        with:
          app-id: "${{ secrets.NUMTERRA_BOT_ID }}"
          private-key: "${{ secrets.NUMTERRA_BOT_PRIVATE_KEY }}"
          owner: NumericalEarth
          repositories: |
            Breeze.jl
            BreezeBenchmarks
      - name: Push results to BreezeBenchmarks repo
        # if: ${{ github.event_name == 'push' }}
        run: |
          # Clone BreezeBenchmarks
          git clone https://numterra-bot:${{ steps.generate_token.outputs.token }}@github.com/NumericalEarth/BreezeBenchmarks.git
          cd BreezeBenchmarks
          DIRNAME="benchmarks/by_branch/${{ github.head_ref || github.ref_name }}/${{ github.sha }}"
          mkdir -vp "${DIRNAME}"
          # Copy over file
          cp -v ../benchmark_results.json "${DIRNAME}/."
          # Push
          git config --global user.name "numterra-bot[bot]"
          git config --global user.email "242463128+numterra-bot[bot]@users.noreply.github.com"
          git add .
          git commit -m'Save benchmark results for commit ${{ github.sha }}'
          git push -u origin main
      - uses: benchmark-action/github-action-benchmark@v1
        with:
          name: Breeze.jl Benchmarks
          tool: "customBiggerIsBetter"
          output-file-path: "github-action-benchmark.json"
          benchmark-data-dir-path: "."
          gh-repository: "github.com/NumericalEarth/BreezeBenchmarks"
          github-token: ${{ github.event.pull_request.head.repo.fork && github.token || steps.generate_token.outputs.token }}
          comment-always: false
          summary-always: true
          alert-threshold: "110%"
          fail-on-alert: false
          auto-push: ${{ github.event_name == 'push' }}
          max-items-in-chart: 50
          comment-on-alert: true
