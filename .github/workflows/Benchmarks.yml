name: Benchmarks

on:
  push:
    branches:
      - main
    tags: "*"
    paths: &paths
      - ".github/workflows/Documentation.yml"
      - "src/**"
      - "Project.toml"
  pull_request:
    paths: *paths

concurrency:
  # Skip intermediate builds: always.
  # Cancel intermediate builds: always.
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  run-benchmarks:
    name: "Run benchmarks"
    permissions:
      actions: write
      contents: write
      pull-requests: read
      statuses: write
    runs-on: aws-linux-nvidia-gpu-l4
    defaults:
      run:
        shell: bash
        working-directory: ./benchmarking
    container:
      # TODO: use image for benchmark environment after this is merged on `main`
      image: 'ghcr.io/numericalearth/breeze-docker-images:docs-julia_1.12.4'
      options: --gpus=all
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v6
      - name: Instantiate benchmarking environment
        shell: julia --color=yes --project {0}
        run: |
          using Pkg
          Pkg.instantiate()
      - name: Run benchmarks
        timeout-minutes: 45
        run:
          earlyoom -m 3 -s 100 -r 300 --prefer 'julia' &
          julia --color=yes --project run_benchmarks.jl --device GPU --size "128^3, 384x384x256, 768x768x256"
      - uses: actions/upload-artifact@v6
        with:
          name: benchmarks-results
          path: benchmarking/benchmark_results.*
          retention-days: 90
      - name: Create job summary
        timeout-minutes: 1
        run:
          cat "benchmark_results.md" >> "${GITHUB_STEP_SUMMARY}"

  publish-benchmarks:
    name: "Publish benchmarks"
    needs: run-benchmarks
    permissions:
      actions: write
      contents: write
      pull-requests: write
      statuses: write
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/download-artifact@v7
      - name: Create JSON for github-action-benchmark
        shell: python
        run: |
          # This step creates a new JSON file to be digested by the
          # `benchmark-action/github-action-benchmark` workflow.
          import json

          with open('benchmark_results.json', 'r') as file:
              orig_data = json.load(file)

          data = []
          for d in orig_data:
              orig_name = d['name'].split('_')
              name = orig_name[0] + '_' + orig_name[2] + '/' + orig_name[3] + '/' + d['metadata']['gpu_name'] + '/' + orig_name[1]
              data.append({'name': name, 'unit': 'points/s', 'value': d['grid_points_per_second']})

          with open("github-action-benchmark.json", "w") as f:
              json.dump(data, f)
      - uses: actions/create-github-app-token@v2
        # if: ${{ github.event_name != 'pull_request' }}
        id: generate_token
        with:
          app-id: "${{ secrets.NUMTERRA_BOT_ID }}"
          private-key: "${{ secrets.NUMTERRA_BOT_PRIVATE_KEY }}"
          owner: NumericalEarth
          repositories: |
            Breeze.jl
            BreezeBenchmarks
      - uses: benchmark-action/github-action-benchmark@v1
        with:
          name: Breeze.jl Benchmarks
          tool: "customBiggerIsBetter"
          output-file-path: "github-action-benchmark.json"
          benchmark-data-dir-path: "."
          gh-repository: "github.com/NumericalEarth/BreezeBenchmarks"
          github-token: ${{ steps.generate_token.outputs.token }}
          # github-token: ${{ github.event_name == 'pull_request' && github.token || steps.generate_token.outputs.token }}
          comment-always: true
          summary-always: true
          alert-threshold: "110%"
          fail-on-alert: false
          auto-push: true
          # auto-push: ${{ github.event_name != 'pull_request' }}
          max-items-in-chart: 50
          # Enable alert commit comment
          comment-on-alert: true
