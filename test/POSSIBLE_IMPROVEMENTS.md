# Possible Improvements to BreezeReactantExt

> **Document Purpose**: Guide performance improvements and future development of Breeze's Reactant integration.
>
> **Key Reference**: Compare against GB-25 (`/Users/tobiwan/Documents/aeolus/GB-25`)

---

## Executive Summary

### Current State

Breeze has working Reactant integration with several workarounds in `ext/BreezeReactantExt/BreezeReactantExt.jl`. All 20 tests pass (see `test/reactant_enzyme.jl`).

**Performance (December 2025)**: Reactant is **3.4√ó faster** than CPU at 256√ó128 grid size. Crossover point is ~128√ó128 ‚Äî below this, CPU is faster. Compilation takes ~150-200s (one-time).

### Why Breeze Needs More Workarounds Than GB-25

| Aspect | GB-25 | Breeze |
|--------|-------|--------|
| **Model** | `HydrostaticFreeSurfaceModel` | `AtmosphereModel` |
| **Pressure Solver** | `SplitExplicitFreeSurface` (no FFT) | `FourierTridiagonalPoissonSolver` (FFT-based) |
| **Array Types** | Real only | Real + Complex (spectral) |
| **Reactant Workarounds** | ~3 simple wrappers | ~10 method overrides |

**Root cause**: Reactant's MLIR lowering fails for complex numbers inside KernelAbstractions kernels, but **works** for complex numbers in pure Julia broadcasts.

### Top 3 Recommended Improvements

| Priority | Improvement | Impact | Effort | Status |
|----------|-------------|--------|--------|--------|
| ‚úÖ **1** | Eliminate 2√ó real tridiagonal split | ~2√ó faster pressure solve | Low | **DONE** |
| ‚úÖ **2** | Profile and benchmark | Data-driven decisions | Low | **DONE** |
| ‚úÖ **3** | In-place FFT optimization | 1.69√ó faster FFT operations | Low | **DONE** |
| ‚ö™ **4** | Reduction caching | Negligible overhead | ‚Äî | **INVESTIGATED** |
| ‚ö™ **5** | Grid specialization | XLA optimizes this | ‚Äî | **INVESTIGATED** |
| üü¢ **6** | Investigate sharded FFT scaling | Multi-GPU readiness | Medium | Pending |

---

## Table of Contents

1. [Background: The Core Problem](#1-background-the-core-problem)
2. [Improvement #1: Eliminate 2√ó Real Split ‚úÖ](#2-improvement-1-eliminate-2-real-split--completed)
3. [Improvement #2: Profile and Benchmark ‚úÖ](#3-improvement-2-profile-and-benchmark--completed)
4. [Improvement #3: In-Place FFT Operations ‚úÖ](#4-improvement-3-in-place-fft-operations--completed)
5. [Improvement #4: Reduction Caching ‚ö™](#5-improvement-4-reduction-caching--investigated---low-priority)
6. [Improvement #5: Grid Specialization ‚ö™](#6-improvement-5-grid-specialization--investigated---no-action-required)
7. [Improvement #6: Parallel Tridiagonal Algorithm](#7-improvement-6-parallel-tridiagonal-algorithm)
8. [Improvement #7: Upstream Reactant Fixes](#8-improvement-7-upstream-reactant-fixes)
9. [Sharding and Distributed Computing](#9-sharding-and-distributed-computing)
10. [GB-25 Patterns to Adopt](#10-gb-25-patterns-to-adopt)
11. [Conclusion](#conclusion)

---

## 1. Background: The Core Problem

### What Works vs What Doesn't

```julia
# ‚úÖ THIS WORKS: Complex numbers in pure Julia broadcasts
sum(real.(complex_array))           # OK
complex_array .* real_array         # OK
ifelse.(condition, complex_a, complex_b)  # OK

# ‚ùå THIS FAILS: Complex numbers in KernelAbstractions kernels
@kernel function my_kernel!(out, inp)
    i, j, k = @index(Global, NTuple)
    out[i, j, k] = real(inp[i, j, k])  # MLIR error!
end
```

**Error message**:
```
'llvm.extractvalue' op operand #0 must be LLVM aggregate type, but got 'complex<f32>'
```

### Verification

Run `test/mwe_reactant_complex_number_mlir.jl` to confirm this behavior.

### Current Workaround Cost Summary

| Workaround | Location (approx lines) | Overhead | Frequency | Status |
|------------|------------------------|----------|-----------|--------|
| ~~2√ó real tridiagonal solve~~ | ~~415-431~~ | ~~2√ó compute~~ | ~~9√ó/iteration~~ | ‚úÖ **ELIMINATED** |
| ~~4 temporary allocations~~ | ~~415-431~~ | ~~GC pressure~~ | ~~Per pressure solve~~ | ‚úÖ **ELIMINATED** |
| ~~FFT copyto! pattern~~ | ~~407-410, 436-438~~ | ~~1.69√ó slower~~ | ~~Per FFT~~ | ‚úÖ **OPTIMIZED** |
| CPU fallback (construction) | 800-920 | Data transfer | Once/simulation | Active |
| Broadcast source term | 619-679 | Minimal | Per pressure solve | Active |
| Broadcast pressure correction | 714-790 | Minimal | Per RK3 substep | Active |
| Field reduction materialization | 133-134 | 1 alloc/reduce | Per CFL check | Active |

---

## 2. Improvement #1: Eliminate 2√ó Real Split ‚úÖ COMPLETED

> **Priority**: ‚úÖ DONE  
> **Effort**: Low (code deletion)  
> **Impact**: ~2√ó faster tridiagonal solve, zero extra allocations
>
> **Implemented**: December 2025 ‚Äî Thomas algorithm now solves directly on complex arrays

### Current Code (Expensive)

```julia
# ext/BreezeReactantExt/BreezeReactantExt.jl lines 415-431
rhs_re = real.(rhs)           # allocate #1
rhs_im = imag.(rhs)           # allocate #2
œï_re = similar(rhs_re)        # allocate #3
œï_im = similar(rhs_im)        # allocate #4

_thomas_solve_z_broadcast!(œï_re, a, b, c, rhs_re, t, Nz)  # solve #1
_thomas_solve_z_broadcast!(œï_im, a, b, c, rhs_im, t, Nz)  # solve #2

œï .= complex.(œï_re, œï_im)     # combine
```

### Why This Can Be Improved

The `_thomas_solve_z_broadcast!` function uses **only broadcasts**, not KA kernels. Therefore, it should work directly on complex arrays.

**Key insight**: The tridiagonal coefficients `a`, `b`, `c` are real (Laplacian eigenvalues). Only `rhs` and `œï` are complex. All operations in the Thomas algorithm are broadcast-based.

### Implemented Code

```julia
# ext/BreezeReactantExt/BreezeReactantExt.jl (current implementation)
# Solve tridiagonal system directly on complex arrays
a = solver.batched_tridiagonal_solver.a
b_arr = solver.batched_tridiagonal_solver.b
c = solver.batched_tridiagonal_solver.c
t = solver.batched_tridiagonal_solver.t

œï = solver.storage
_thomas_solve_z_broadcast!(œï, a, b_arr, c, rhs, t, Nz)  # ONE complex solve
```

### Verification Results ‚úÖ

All tests passed (December 2025):
```
Test Summary:        | Pass  Total      Time
Reactant + Enzyme AD |   20     20  15m25.7s
```

### Verification Checklist

- [x] `test/reactant_enzyme.jl` passes (20/20 tests)
- [x] Pressure solve produces correct results
- [x] No new allocations in pressure solve path (4 allocations eliminated)
- [x] No MLIR compilation errors

### Achieved Benefit

| Metric | Before | After |
|--------|--------|-------|
| Tridiagonal solves per pressure solve | 2 | **1** |
| Allocations per pressure solve | 4 | **0** |
| Memory bandwidth | 6√ó array copies | **0** |

---

## 3. Improvement #2: Profile and Benchmark ‚úÖ COMPLETED

> **Priority**: ‚úÖ DONE  
> **Effort**: Low  
> **Impact**: Data-driven optimization decisions enabled
>
> **Implemented**: December 2025 ‚Äî Benchmark script created and results collected

### Benchmark Results (December 2025)

Using `test/benchmark_reactant.jl` on CPU backend (XLA CPU vs native Julia):

| Grid Size | CPU (ms/step) | Reactant (ms/step) | Speedup | Compile Time | Steps to Amortize |
|-----------|---------------|---------------------|---------|--------------|-------------------|
| 32√ó32 | 2.55 | 7.51 | **0.34√ó** (slower) | 149s | ‚Äî |
| 64√ó64 | 4.65 | 7.62 | **0.61√ó** (slower) | 167s | ‚Äî |
| 128√ó128 | 11.89 | 9.16 | **1.3√ó** | 194s | 70,884 |
| 256√ó128 | 39.62 | 11.73 | **3.4√ó** | 196s | 7,025 |

### Key Findings

1. **Crossover point**: Reactant becomes faster than CPU around **128√ó128** grid size
2. **Scaling advantage**: Reactant scales better with grid size (sublinear vs linear)
3. **Compilation cost**: ~150-200 seconds regardless of grid size (one-time)
4. **Variance**: Reactant has lower variance (more consistent timing)

### Interpretation

| Observation | Implication |
|-------------|-------------|
| Small grids are slower | Reactant overhead dominates; use CPU for prototyping |
| Large grids are faster | XLA optimizations pay off; use Reactant for production |
| Sublinear scaling | Suggests good parallelization in compiled code |
| High compile time | Compilation is a one-time cost; amortizes over long runs |

### Recommendations Based on Benchmarks

1. **For development**: Use CPU (`RectilinearGrid(CPU(); ...)`) ‚Äî faster iteration
2. **For production**: Use Reactant for grids ‚â• 128√ó128
3. **GPU backend**: Expected to show even larger speedups (not yet tested)
4. **Compilation caching**: Investigate if XLA caches compiled kernels between sessions

### Benchmark Script Location

The benchmark script is at `test/benchmark_reactant.jl`. Usage:

```bash
# Default 32√ó32 grid
julia --project=test test/benchmark_reactant.jl

# Custom grid size
BENCH_NX=128 BENCH_NZ=128 julia --project=test test/benchmark_reactant.jl
```

### Future Work

- [ ] Profile with `Reactant.with_profiler()` to identify bottlenecks
- [ ] Test GPU backend (requires NVIDIA GPU)
- [ ] Investigate compilation caching between Julia sessions
- [ ] Test 3D grids (512√ó512√ó64 typical for LES)

---

## 4. Improvement #3: In-Place FFT Operations ‚úÖ COMPLETED

> **Priority**: ‚úÖ DONE  
> **Effort**: Low  
> **Impact**: 1.69√ó faster FFT operations
>
> **Implemented**: December 2025 ‚Äî Changed `copyto!` to broadcast assignment

### Previous Code (Slower)

```julia
rhs_fft = AbstractFFTs_mod.fft(rhs, periodic_dims)  # allocates temporary
copyto!(rhs, rhs_fft)                                # copies back
```

### New Code (Faster)

```julia
rhs .= AbstractFFTs_mod.fft(rhs, periodic_dims)  # XLA fuses this!
```

### Investigation Results

Created `test/mwe_reactant_inplace_fft.jl` to investigate FFT patterns:

| Pattern | Description | Time (128√ó128) |
|---------|-------------|----------------|
| A | `copyto!(A, fft(A))` | 0.092 ms |
| B | `A .= fft(A)` | 0.054 ms |
| C | `return fft(A)` | 0.023 ms |

**Key findings**:
1. `fft!()` does NOT exist for Reactant (pointer conversion fails)
2. Single integer dims fail (`fft(A, 1)`), but tuples work (`fft(A, (1,))`)
3. XLA fuses broadcast assignment, eliminating intermediate allocations
4. Broadcast pattern is **1.69√ó faster** than `copyto!`

### XLA HLO Analysis

The optimized HLO shows a single fused `stablehlo.fft` operation:
```
func.func @main(%arg0: tensor<4x4xcomplex<f64>>) -> tensor<4x4xcomplex<f64>> {
  %0 = stablehlo.fft %arg0, type = FFT, length = [4] : tensor<4x4xcomplex<f64>>
  return %0 : tensor<4x4xcomplex<f64>>
}
```

No intermediate allocation or copy operations ‚Äî XLA fully optimizes this.

### Code Changes

Updated `ext/BreezeReactantExt/BreezeReactantExt.jl`:
- Line 407-410: Changed forward FFT from `copyto!` to broadcast
- Line 436-438: Changed inverse FFT from `copyto!` to broadcast

### Verification

All 20 tests pass after the change.

---

## 5. Improvement #4: Reduction Caching ‚ö™ INVESTIGATED - LOW PRIORITY

> **Priority**: ‚ö™ VERY LOW  
> **Effort**: Medium  
> **Impact**: Negligible after JIT warmup
>
> **Investigated**: December 2025 ‚Äî Overhead is minimal, not worth the complexity

### Investigation Results

Created `test/mwe_reactant_reduction_caching.jl` to measure Field(op) overhead:

| Call # | Field(op) Time |
|--------|----------------|
| 1st (cold) | 411 ms |
| 2nd+ (warm) | 0.04-0.05 ms |

**Key Findings**:
1. First call is slow due to JIT compilation (~411 ms)
2. Subsequent calls are fast (~0.04 ms) ‚Äî negligible overhead
3. `minimum(op)` is actually faster than `minimum(precomputed_field)`
4. In Reactant @compile context, XLA optimizes allocations

### Why No Action Required

```
After JIT warmup:
  Field(op) creation: ~0.04 ms
  Compared to:
    FFT operations:   ~0.05 ms (from Improvement #3)
    Time step total:  ~10 ms (from benchmarks)
    
Field(op) overhead: ~0.4% of time step ‚Äî negligible
```

### Original Proposed Solution (Not Implemented)

```julia
# Would add complexity for negligible benefit
const _REDUCTION_CACHE = Dict{UInt64, Any}()

function _get_scratch_field(op::AbstractOperation)
    # ...caching logic...
end
```

### Conclusion

- Caching adds complexity with uncertain benefit
- JIT warmup handles the overhead naturally
- XLA optimization eliminates allocations in traced code
- **No code changes needed**

---

## 6. Improvement #5: Grid Specialization ‚ö™ INVESTIGATED - NO ACTION REQUIRED

> **Priority**: ‚ö™ VERY LOW  
> **Effort**: Low  
> **Impact**: Negligible ‚Äî XLA optimizes this away
>
> **Investigated**: December 2025 ‚Äî Runtime checks are effectively free

### Investigation Results

Created `test/mwe_reactant_grid_specialization.jl` to compare patterns:

| Pattern | Time (100k calls) | Per-call |
|---------|-------------------|----------|
| Runtime check | 61.36 ms | 0.61 ¬µs |
| Dispatch | 46.59 ms | 0.47 ¬µs |

**Dispatch is 1.32√ó faster** in microbenchmarks, BUT:

### Why No Action Required

1. **Absolute overhead is negligible**: 0.14 ¬µs difference per call
2. **XLA eliminates the check**: During @compile, topology is constant
3. **Code clarity**: Runtime checks are self-documenting
4. **Complexity trade-off**: Dispatch requires 2√ó more method definitions

```
Comparison to actual work:
  Runtime check overhead: 0.61 ¬µs
  FFT operation:          50 ¬µs
  Time step total:        10,000 ¬µs
  
  Overhead: 0.006% of time step ‚Äî completely negligible
```

### XLA Optimization Insight

When Reactant traces the function:
```julia
topo = topology(grid)     # Returns CONCRETE tuple at trace time
if topo[2] !== Flat       # Becomes CONSTANT check
    error(...)            # Dead code ‚Äî eliminated by XLA
end
```

The runtime check has **zero overhead** in compiled XLA code.

### Conclusion

- Current runtime checks are clear and effective
- XLA dead-code elimination handles optimization
- Dispatch adds complexity with no meaningful benefit
- **No code changes needed**

---

## 7. Improvement #6: Parallel Tridiagonal Algorithm

> **Priority**: ‚ö™ VERY LOW (only if profiling shows bottleneck)  
> **Effort**: High  
> **Impact**: Better GPU utilization for shallow grids

### When This Matters

The Thomas algorithm is O(Nz) sequential. For grids with small Nz (e.g., 50-100 levels), this may not fully utilize GPU parallelism.

### Alternative: Cyclic Reduction

```julia
# O(log Nz) parallel steps instead of O(Nz) sequential
function cyclic_reduction_solve!(œï, a, b, c, rhs, Nz)
    # Forward reduction: log2(Nz) parallel phases
    for level in 1:ceil(Int, log2(Nz))
        stride = 2^level
        # All k values processed in parallel
        # Each eliminates one variable
    end
    
    # Back substitution: log2(Nz) parallel phases
    for level in ceil(Int, log2(Nz)):-1:1
        # Parallel solve
    end
end
```

### Trade-offs

| Algorithm | Sequential Steps | Best For |
|-----------|------------------|----------|
| Thomas | O(Nz) | CPU, deep grids |
| Cyclic Reduction | O(log Nz) | GPU, shallow grids |

### Recommendation

**Wait for profiling data** before implementing. The Thomas algorithm may be fast enough.

---

## 8. Improvement #7: Upstream Reactant Fixes

> **Priority**: ‚ö™ MONITORING  
> **Effort**: None (wait for upstream)  
> **Impact**: Would eliminate most workarounds

### When Reactant Fixes Complex Numbers in KA Kernels

We can remove:
1. FFT planning no-ops
2. Thomas algorithm workaround
3. Source term broadcast workaround
4. Pressure correction broadcast workaround

### How to Check

```bash
# Run the MWE periodically
julia --project=test test/mwe_reactant_complex_number_mlir.jl
```

If the "Trigger" section starts passing, upstream is fixed.

### Tracking

- Watch [Reactant.jl issues](https://github.com/EnzymeAD/Reactant.jl/issues)
- Search for "complex" or "llvm.extractvalue"

---

## 9. Sharding and Distributed Computing

> **This section covers multi-GPU/TPU considerations for future Breeze scalability.**

### How GB-25 Does Distributed Computing

GB-25 uses **Reactant's native sharding** (not MPI):

```julia
# GB-25/sharding/simple_sharding_problem.jl
GordonBell25.initialize()  # XLA distributed runtime
ndevices = length(Reactant.devices())

arch = Oceananigans.Distributed(
    Oceananigans.ReactantState();
    partition = Partition(Rx, Ry, 1)
)
```

### Oceananigans Sharding Integration

From `OceananigansReactantExt/Grids/sharded_grids.jl`:

```julia
# Data sharded along x and y
xysharding = Sharding.DimsSharding(arch.connectivity, (1, 2), (:x, :y))

# Z replicated across all devices
replicate = Sharding.Replicated(arch.connectivity)
z = sharded_z_direction(grid.z; sharding=replicate)
```

### The FFT Sharding Challenge

Breeze's `FourierTridiagonalPoissonSolver` requires:

```
Physical space ‚Üí FFT(x,y) ‚Üí Spectral space ‚Üí Tridiagonal solve(z) ‚Üí IFFT(x,y) ‚Üí Physical space
```

**Problem**: If data is sharded along x or y, FFT requires **all-to-all communication**.

**Why GB-25 avoids this**: Uses `SplitExplicitFreeSurface` ‚Äî no FFTs, only local stencils.

### Sharding Strategies for Breeze

| Strategy | Description | Trade-off |
|----------|-------------|-----------|
| **A: Shard x only** | FFT in y is local | Limited parallelism |
| **B: Pencil decomposition** | Transpose before FFT | Communication overhead |
| **C: Alternative solver** | Multigrid/CG | More iterations, no FFT |

### Investigation Checklist

1. **Test basic sharding**
   ```julia
   arch = Distributed(ReactantState(); partition=Partition(2, 1, 1))
   grid = RectilinearGrid(arch; size=(64, 1, 32), ...)
   model = AtmosphereModel(; grid)
   # Does it compile?
   ```

2. **Profile FFT communication**
   ```julia
   Reactant.with_profiler("sharded_profile") do
       compiled_step!(model, Œît)
   end
   # Look for AllToAll or CollectivePermute operations
   ```

3. **Compare scaling**
   ```julia
   # 1 device vs 4 devices ‚Äî is there speedup?
   ```

### Compatibility Status

| Feature | Status | Notes |
|---------|--------|-------|
| Single-device ReactantState | ‚úÖ Working | All tests pass |
| Distributed(ReactantState) | ‚ö†Ô∏è Untested | Needs investigation |
| Sharded FFT | ‚ùì Unknown | May require all-to-all |
| Sharded pressure correction | ‚úÖ Should work | Broadcast-based |

---

## 10. GB-25 Patterns to Adopt

### Pattern 1: Wrapper Functions for Tracing

GB-25 wraps functions to ensure proper tracing:

```julia
# GB-25/src/timestepping_utils.jl
function time_step!(model)
    Œît = model.clock.last_Œît + 0  # `+ 0` ensures Œît is traced
    Oceananigans.TimeSteppers.time_step!(model, Œît)
    return nothing
end
```

**Why `+ 0`**: Forces `Œît` to be a new traced value, avoiding issues with accessing struct fields.

### Pattern 2: Separate `first_time_step!` and `loop!`

```julia
# First step may need different initialization
compiled_first! = @compile first_time_step!(model)

# Main loop uses traced iteration
Ninner = ConcreteRNumber(100)
compiled_loop! = @compile loop!(model, Ninner)

# Execution
compiled_first!(model)
compiled_loop!(model, Ninner)
```

### Pattern 3: `@trace` with `track_numbers=false`

```julia
function loop!(model, Ninner)
    Œît = model.clock.last_Œît + 0
    @trace track_numbers=false for _ = 1:Ninner
        time_step!(model, Œît)
    end
end
```

**Why `track_numbers=false`**: Prevents Reactant from trying to trace integer loop indices, which can cause issues.

### Pattern 4: TreeSharding for Nested Structures

```julia
# GB-25/src/sharding_utils.jl
struct TreeSharding{S} <: Sharding.AbstractSharding
    sharding::S
end

# Propagates sharding through nested field access
Base.getproperty(t::TreeSharding, x) = t
```

Useful for complex models like `AtmosphereModel` with deeply nested fields.

---

## Quick Reference: File Locations

### Breeze Extension Code

| Component | File | Lines (approx) |
|-----------|------|----------------|
| Field reductions | `ext/BreezeReactantExt/BreezeReactantExt.jl` | 69-171 |
| CartesianIndex fix | same | 183-187 |
| Thomas algorithm | same | 271-339 |
| Poisson solver | same | 341-451 |
| FFT planning | same | 454-489 |
| RK3 time_step! | same | 521-588 |
| Source term | same | 619-679 |
| Pressure correction | same | 714-790 |
| CPU fallback | same | 801-919 |

### Test Files

| Purpose | File |
|---------|------|
| Main test suite | `test/reactant_enzyme.jl` |
| **Benchmark script** | `test/benchmark_reactant.jl` |
| **In-place FFT MWE** | `test/mwe_reactant_inplace_fft.jl` |
| **Reduction caching MWE** | `test/mwe_reactant_reduction_caching.jl` |
| **Grid specialization MWE** | `test/mwe_reactant_grid_specialization.jl` |
| FFT planning MWE | `test/mwe_reactant_fft_planning.jl` |
| Complex number MWE | `test/mwe_reactant_complex_number_mlir.jl` |
| Solver compile MWE | `test/mwe_reactant_fourier_tridiagonal_solver_compile.jl` |
| Issue tracker | `test/REACTANT_ISSUES.md` |
| Improvement tracker | `test/POSSIBLE_IMPROVEMENTS.md` (this file) |

### GB-25 Reference

| Component | File |
|---------|------|
| Time stepping | `/Users/tobiwan/Documents/aeolus/GB-25/src/timestepping_utils.jl` |
| Sharding utils | `/Users/tobiwan/Documents/aeolus/GB-25/src/sharding_utils.jl` |
| Model setup | `/Users/tobiwan/Documents/aeolus/GB-25/src/baroclinic_instability_model.jl` |
| Sharded run | `/Users/tobiwan/Documents/aeolus/GB-25/sharding/sharded_baroclinic_instability_simulation_run.jl` |

---

## Conclusion

### Completed ‚úÖ

1. **Improvement #1** (eliminate 2√ó real split) ‚Äî **DONE** (December 2025)
   - Thomas algorithm now solves directly on complex arrays
   - 2√ó faster tridiagonal solve, 0 allocations (was 4)

2. **Improvement #2** (profile and benchmark) ‚Äî **DONE** (December 2025)
   - Benchmark script created at `test/benchmark_reactant.jl`
   - Key finding: Reactant is **3.4√ó faster** than CPU at 256√ó128 grid size
   - Crossover point: ~128√ó128 (below this, CPU is faster)
   - Compilation time: ~150-200s (one-time cost)

3. **Improvement #3** (in-place FFT) ‚Äî **DONE** (December 2025)
   - Changed `copyto!(A, fft(A))` to `A .= fft(A)` for better XLA fusion
   - 1.69√ó faster FFT operations
   - MWE created at `test/mwe_reactant_inplace_fft.jl`

### Investigated - No Action Required ‚ö™

4. **Improvement #4** (reduction caching) ‚Äî **INVESTIGATED** (December 2025)
   - Field(op) overhead is ~0.04 ms after JIT warmup ‚Äî negligible
   - XLA optimizes allocations in traced code
   - MWE created at `test/mwe_reactant_reduction_caching.jl`

5. **Improvement #5** (grid specialization) ‚Äî **INVESTIGATED** (December 2025)
   - Runtime check overhead is ~0.6 ¬µs ‚Äî 0.006% of time step
   - XLA dead-code eliminates runtime checks anyway
   - MWE created at `test/mwe_reactant_grid_specialization.jl`

### Short-Term (This Month)

- **Test basic sharding** ‚Äî verify Breeze works with `Distributed(ReactantState())`
- **Test GPU backend** ‚Äî expected to show even larger speedups

### Medium-Term (This Quarter)

- **Investigate sharded FFT performance** ‚Äî is it a bottleneck?
- **Consider alternative solvers** if FFT sharding is problematic

### Long-Term (Monitor)

- **Watch upstream Reactant** ‚Äî complex number fix would simplify everything
- **Evaluate multigrid** ‚Äî if FFT scaling is poor at large scale
